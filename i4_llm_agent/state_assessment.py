# === START OF FILE i4_llm_agent/state_assessment.py ===
# [[START MODIFIED state_assessment.py - Remove Prompt Log]]
# i4_llm_agent/state_assessment.py

import logging
import json
import asyncio
from typing import Dict, Any, Optional, Callable, Coroutine, List, Tuple, Union

# Attempt import for history formatting (optional context for parser LLM)
try:
    from .history import format_history_for_llm, get_recent_turns, DIALOGUE_ROLES
except ImportError:
    DIALOGUE_ROLES = ["user", "assistant"]
    def get_recent_turns(*args, **kwargs): return []
    def format_history_for_llm(*args, **kwargs): return "[History Unavailable]"
    logging.getLogger(__name__).warning("Failed to import history utils in state_assessment.py")

logger = logging.getLogger(__name__) # i4_llm_agent.state_assessment

# --- Constants ---

# Placeholders for the Unified State Assessment prompt
PREVIOUS_STATE_PLACEHOLDER = "{previous_state_json}"
CURRENT_QUERY_PLACEHOLDER = "{current_user_query}"
ASSISTANT_RESPONSE_PLACEHOLDER = "{current_assistant_response}"
HISTORY_CONTEXT_PLACEHOLDER = "{recent_history_str}"
WEATHER_PROPOSAL_PLACEHOLDER = "{weather_proposal_text}"

DEFAULT_UNIFIED_STATE_ASSESSMENT_PROMPT_TEXT = f"""
[[SYSTEM ROLE: Unified World & Scene State Assessor]]

**Objective:** Analyze the complete context of the **Current Turn** (Previous State, User Input, Assistant Response, Weather Proposal) to determine the definitive **New State** (World settings and Scene description) that should be carried forward to the *next* turn. Output ONLY a single, valid JSON object containing this complete new state.

**Inputs:**

1.  **Previous State (JSON Object):** The world and scene state *before* the current turn began.
    ```json
    {PREVIOUS_STATE_PLACEHOLDER}
    ```
    *(Contains: previous_day, previous_time_of_day, previous_weather, previous_season, previous_scene_keywords, previous_scene_description)*

2.  **Current User Input:** The exact input provided by the user for this turn. Pay close attention to `STORY:` commands which may dictate state changes.
    ```text
    {CURRENT_QUERY_PLACEHOLDER}
    ```

3.  **Current Assistant Response:** The narrative response generated by the assistant for this turn. This reflects the events that occurred.
    ```text
    {ASSISTANT_RESPONSE_PLACEHOLDER}
    ```

4.  **Recent History (Context Only):** The last few turns for conversational context. **DO NOT** extract state changes directly from here; use it only to understand the flow leading to the Current User Input/Assistant Response.
    ```text
    {HISTORY_CONTEXT_PLACEHOLDER}
    ```

5.  **Weather Proposal (From Hint System):** A suggestion for the next likely weather state.
    ```text
    {WEATHER_PROPOSAL_PLACEHOLDER}
    ```
    *(Format: "Proposal: From [Previous] to [New]" or "[No Proposal Provided]")*

**Instructions:**

1.  **Analyze Full Turn:** Consider all inputs together to understand what happened during this turn.
2.  **Prioritize User Commands:** Explicit `STORY:` commands in the `Current User Input` generally take precedence for dictating state changes (especially time/scene shifts).
3.  **Determine New World State:**
    *   **`new_day` & `new_time_of_day`**: Calculate based on `Previous State` and progression indicated in `Current User Input` (commands) or `Current Assistant Response` (narrative). Increment day if needed (usually resets time to Morning). If no change indicated, return previous values.
    *   **`new_weather`**:
        *   **Prioritize the `Weather Proposal`:** If a proposal is provided (not "[No Proposal Provided]"), use its suggested `new_weather` value **UNLESS** the `Current User Input` (e.g., `STORY: The sun came out`) or the `Current Assistant Response` *explicitly and strongly contradicts* it (e.g., proposal is "Rainy" but response says "clear skies").
        *   If no proposal is provided OR if the proposal is contradicted by explicit commands/narrative, determine the weather based primarily on the `Current Assistant Response` descriptions.
        *   If the response is consistent with `previous_weather` and no proposal/command contradicts, keep `previous_weather`.
        *   If no weather is mentioned anywhere and no proposal exists, generally keep `previous_weather`.
    *   **`new_season`**: Keep `previous_season` unless explicitly changed by command or long narrative time jump.

# === START MODIFIED INSTRUCTION 4 ===
4.  **Determine New Scene State:**
    *   **Analyze Narrative Context:** Review `Current User Input` (especially `STORY:` commands) and `Current Assistant Response` for explicit indications of a fundamental location change (e.g., traveling, entering a new distinct area).
    *   **Assess Location Change:** Did the narrative context clearly indicate a move to a *different location* compared to the `previous_scene_description`?
    *   **If Location Changed:**
        *   Generate a **new `new_scene_description`** based primarily on the static environmental details described for the *new* location in the `Current Assistant Response`. Aim for a concise description (typically 2-4 static sentences, no actions/dialogue).
        *   Generate **new `new_scene_keywords`** (3-5 keywords) reflecting this *new* description.
        *   Set `scene_changed_flag: true`.
    *   **If Location Is The Same (or unclear change):**
        *   Take the `previous_scene_description` as the base text.
        *   Identify **specific, persistent environmental changes** described in the `Current Assistant Response` that affect this location (e.g., "the northern wall now lies in rubble," "rain streaks the windows," "a permanent magical ward glows on the door," "the campfire has died out"). Ignore transient actions or dialogue.
        *   **Modify the base description** to accurately incorporate *only* these persistent environmental changes. Retain the rest of the base description that remains accurate. Ensure the final `new_scene_description` is static and concise.
        *   Generate `new_scene_keywords` (3-5 keywords) reflecting the *final modified* description.
        *   Set `scene_changed_flag: false` (because the core location remains the same, even if altered).
# === END MODIFIED INSTRUCTION 4 ===

5.  **Output Format:** Respond ONLY with a single, valid JSON object containing the complete new state. Ensure all keys listed below are present.

    ```json
    {{
      "new_day": <integer>,
      "new_time_of_day": "<string: Morning|Afternoon|Evening|Night>",
      "new_weather": "<string>",
      "new_season": "<string>",
      "new_scene_keywords": ["<string>", "<string>", ...],
      "new_scene_description": "<string>",
      "scene_changed_flag": <boolean: true|false>
    }}
    ```

**JSON Output Only:**
"""

# --- Helper Function ---

def _format_unified_state_assessment_prompt(
    template: str,
    previous_world_state: Dict[str, Any],
    previous_scene_state: Dict[str, Any],
    current_user_query: str,
    current_assistant_response: str,
    recent_history_str: str,
    weather_proposal: Optional[Dict[str, Optional[str]]] = None
) -> str:
    """Formats the prompt for the Unified State Assessment LLM."""
    func_logger = logging.getLogger(__name__ + '._format_unified_state_assessment_prompt')
    if not template or not isinstance(template, str):
        return "[Error: Invalid Template for Unified State Assessment]"

    previous_state_combined = {
        "previous_day": previous_world_state.get("day", 1),
        "previous_time_of_day": previous_world_state.get("time_of_day", "Morning"),
        "previous_weather": previous_world_state.get("weather", "Clear"),
        "previous_season": previous_world_state.get("season", "Summer"),
        "previous_scene_keywords": previous_scene_state.get("keywords", []),
        "previous_scene_description": previous_scene_state.get("description", "")
    }

    previous_state_json_str = "[Error: Failed to serialize previous state]"
    try:
        previous_state_json_str = json.dumps(previous_state_combined, indent=2)
    except Exception as e_ser:
        func_logger.error(f"Failed to serialize previous state to JSON: {e_ser}")

    proposal_text = "[No Proposal Provided]"
    if isinstance(weather_proposal, dict) and \
       "previous_weather" in weather_proposal and \
       "new_weather" in weather_proposal:
        prev_w = weather_proposal.get("previous_weather") or "Unknown"
        new_w = weather_proposal.get("new_weather") or "Unknown"
        if new_w != prev_w:
            proposal_text = f"Proposal: From '{prev_w}' to '{new_w}'"
        else:
            proposal_text = f"Proposal: No change suggested (Remain '{prev_w}')"

    safe_user_query = str(current_user_query)
    safe_assistant_response = str(current_assistant_response)
    safe_history = str(recent_history_str)

    try:
        formatted_prompt = template.replace(PREVIOUS_STATE_PLACEHOLDER, previous_state_json_str)
        formatted_prompt = formatted_prompt.replace(CURRENT_QUERY_PLACEHOLDER, safe_user_query)
        formatted_prompt = formatted_prompt.replace(ASSISTANT_RESPONSE_PLACEHOLDER, safe_assistant_response)
        formatted_prompt = formatted_prompt.replace(HISTORY_CONTEXT_PLACEHOLDER, safe_history)
        formatted_prompt = formatted_prompt.replace(WEATHER_PROPOSAL_PLACEHOLDER, proposal_text)

        if any(ph in formatted_prompt for ph in [
            PREVIOUS_STATE_PLACEHOLDER, CURRENT_QUERY_PLACEHOLDER,
            ASSISTANT_RESPONSE_PLACEHOLDER, HISTORY_CONTEXT_PLACEHOLDER,
            WEATHER_PROPOSAL_PLACEHOLDER
        ]):
            func_logger.warning(f"Potential placeholder missed during .replace() formatting for unified state prompt.")

        return formatted_prompt
    except Exception as e:
        func_logger.error(f"Error formatting unified state assessment prompt: {e}", exc_info=True)
        return f"[Error formatting unified state assessment prompt: {type(e).__name__}]"

# --- Core Logic ---

async def update_state_via_full_turn_assessment(
    session_id: str,
    previous_world_state: Dict[str, Any],
    previous_scene_state: Dict[str, Any],
    current_user_query: str,
    assistant_response_text: str,
    history_messages: List[Dict],
    llm_call_func: Callable[..., Coroutine[Any, Any, Tuple[bool, Union[str, Dict]]]],
    state_assessment_llm_config: Dict[str, Any], # Needs URL, Key, Temp etc.
    logger_instance: Optional[logging.Logger] = None,
    event_emitter: Optional[Callable] = None, # For potential status updates
    weather_proposal: Optional[Dict[str, Optional[str]]] = None
) -> Dict[str, Any]:
    """
    Uses a single LLM call to assess the full turn context (previous state,
    user query, assistant response, **and weather proposal**) to determine
    the complete new world and scene state.

    Args:
        session_id: The session ID for logging.
        previous_world_state: Dict containing the state before this turn.
        previous_scene_state: Dict containing the scene before this turn.
        current_user_query: The user's input for the current turn.
        assistant_response_text: The assistant's response for the current turn.
        history_messages: Recent dialogue history for context.
        llm_call_func: The async function wrapper to call the LLM.
        state_assessment_llm_config: Dict containing 'url', 'key', 'temp', 'prompt_template'
                                     for the State Assessment LLM.
                                     ***Ensure this contains the desired LLM endpoint/key***
        logger_instance: Optional logger instance.
        event_emitter: Optional callable for status updates.
        weather_proposal: Optional dictionary containing the weather proposal
                          from the hint system (e.g., {"previous_weather": ..., "new_weather": ...}).

    Returns:
        A dictionary containing the complete new state, matching the structure
        specified in the prompt's output format instructions. Returns a
        dictionary based on the *previous* state in case of errors.
    """
    func_logger = logger_instance or logger
    caller_info = f"UnifiedStateAssess_{session_id}"

    fallback_state = {
        "new_day": previous_world_state.get("day", 1),
        "new_time_of_day": previous_world_state.get("time_of_day", "Morning"),
        "new_weather": previous_world_state.get("weather", "Clear"),
        "new_season": previous_world_state.get("season", "Summer"),
        "new_scene_keywords": previous_scene_state.get("keywords", []),
        "new_scene_description": previous_scene_state.get("description", ""),
        "scene_changed_flag": False
    }

    # --- Prepare Inputs ---
    if not isinstance(history_messages, list): history_messages = []; func_logger.warning(f"[{caller_info}] history_messages not list.")
    history_context_turns = get_recent_turns(history_messages, 4, DIALOGUE_ROLES, exclude_last=False)
    recent_history_str = format_history_for_llm(history_context_turns)

    prompt_template = state_assessment_llm_config.get('prompt_template', DEFAULT_UNIFIED_STATE_ASSESSMENT_PROMPT_TEXT)
    if not prompt_template or not isinstance(prompt_template, str):
         prompt_template = DEFAULT_UNIFIED_STATE_ASSESSMENT_PROMPT_TEXT
         func_logger.warning(f"[{caller_info}] Invalid prompt template in config, using default unified assessment prompt (v2 with weather proposal).")

    prompt_text = _format_unified_state_assessment_prompt(
        template=prompt_template,
        previous_world_state=previous_world_state,
        previous_scene_state=previous_scene_state,
        current_user_query=current_user_query,
        current_assistant_response=assistant_response_text,
        recent_history_str=recent_history_str,
        weather_proposal=weather_proposal
    )

    if not prompt_text or prompt_text.startswith("[Error:"):
        func_logger.error(f"[{caller_info}] Failed format prompt: {prompt_text}. Returning previous state.")
        return fallback_state

    # --- Get LLM Config ---
    llm_url = state_assessment_llm_config.get('url')
    llm_key = state_assessment_llm_config.get('key')
    llm_temp = state_assessment_llm_config.get('temp', 0.3)

    if not llm_url or not llm_key:
        func_logger.error(f"[{caller_info}] State Assessment LLM URL/Key missing in config. Returning previous state.")
        return fallback_state
    if not llm_call_func or not asyncio.iscoroutinefunction(llm_call_func):
        func_logger.error(f"[{caller_info}] Invalid llm_call_func provided. Returning previous state.")
        return fallback_state


    # --- LLM Call ---
    payload = {"contents": [{"parts": [{"text": prompt_text}]}]}
    func_logger.info(f"[{caller_info}] Calling Unified State Assessment LLM (v2 prompt with weather proposal)...")
    # <<< START MODIFICATION (Prompt Log Removed) >>>
    # func_logger.debug(f"[{caller_info}] State Assessment Prompt (first 500 chars):\n-------\n{prompt_text[:500]}...\n-------")
    # <<< END MODIFICATION >>>

    success = False
    response_or_error = "LLM Call Not Attempted"
    try:
        success, response_or_error = await llm_call_func(
            api_url=llm_url,
            api_key=llm_key,
            payload=payload,
            temperature=llm_temp,
            timeout=60, # Adjust timeout as needed
            caller_info=caller_info
        )
    except Exception as e_call:
        func_logger.error(f"[{caller_info}] Exception during LLM call: {e_call}", exc_info=True)
        success = False
        response_or_error = f"LLM Call Exception: {type(e_call).__name__}"

    # --- Process Response ---
    if success and isinstance(response_or_error, str):
        llm_output_text = response_or_error.strip()
        func_logger.debug(f"[{caller_info}] State Assessment Raw Output (first 500 chars):\n{llm_output_text[:500]}...")

        try:
            if llm_output_text.startswith("```json"): llm_output_text = llm_output_text[7:]
            if llm_output_text.endswith("```"): llm_output_text = llm_output_text[:-3]
            llm_output_text = llm_output_text.strip()

            if not llm_output_text:
                func_logger.warning(f"[{caller_info}] Unified state LLM returned empty string. Returning previous state.")
                return fallback_state

            parsed_json = json.loads(llm_output_text)

            # --- Validate structure and types ---
            required_keys = {
                "new_day": int,
                "new_time_of_day": str,
                "new_weather": str,
                "new_season": str,
                "new_scene_keywords": list,
                "new_scene_description": str,
                "scene_changed_flag": bool
            }
            valid = True
            if not isinstance(parsed_json, dict):
                func_logger.warning(f"[{caller_info}] Parsed JSON is not a dictionary. Output: {llm_output_text[:200]}...")
                valid = False
            else:
                for key, expected_type in required_keys.items():
                    if key not in parsed_json:
                        func_logger.warning(f"[{caller_info}] Parsed JSON missing required key: '{key}'. Output: {llm_output_text[:200]}...")
                        valid = False
                        break
                    current_value = parsed_json[key]
                    if current_value is None:
                         func_logger.warning(f"[{caller_info}] Parsed JSON key '{key}' has None value, expected {expected_type.__name__}. Output: {llm_output_text[:200]}...")
                         valid = False
                         break

                    if not isinstance(current_value, expected_type):
                        if key == "new_scene_keywords" and expected_type == list:
                            if not all(isinstance(item, str) for item in current_value):
                                func_logger.warning(f"[{caller_info}] Parsed JSON key '{key}' contains non-string elements. Value: {current_value}. Output: {llm_output_text[:200]}...")
                                valid = False
                                break
                        else:
                             func_logger.warning(f"[{caller_info}] Parsed JSON key '{key}' has wrong type. Expected {expected_type.__name__}, got {type(current_value).__name__}. Value: {current_value}. Output: {llm_output_text[:200]}...")
                             valid = False
                             break

            if valid:
                func_logger.info(f"[{caller_info}] Unified state LLM returned valid JSON structure.")
                return parsed_json
            else:
                func_logger.warning(f"[{caller_info}] Unified state LLM output failed structure/type validation. Returning previous state.")
                return fallback_state

        except json.JSONDecodeError as e_json:
            func_logger.error(f"[{caller_info}] Failed parse JSON response: {e_json}. Output: {llm_output_text[:200]}... Returning previous state.")
            return fallback_state
        except Exception as e_parse:
            func_logger.error(f"[{caller_info}] Error processing parsed JSON: {e_parse}. Output: {llm_output_text[:200]}... Returning previous state.", exc_info=True)
            return fallback_state
    else:
        error_details = str(response_or_error)
        func_logger.warning(f"[{caller_info}] Unified state LLM call failed or returned invalid type. Error: '{error_details}'. Returning previous state.")
        return fallback_state

# [[END MODIFIED state_assessment.py - Remove Prompt Log]]
# === END OF FILE i4_llm_agent/state_assessment.py ===